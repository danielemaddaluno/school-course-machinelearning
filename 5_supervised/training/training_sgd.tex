\subsection[La discesa stocastica del gradiente (SGD e SGD mini-batch)]{La discesa stocastica del gradiente (SGD e SGD mini-batch)}
\begin{frame}
	
	\frametitle{La discesa stocastica del gradiente: SGD e SGD mini-batch}
	
	\begin{block}{Due versioni più efficienti del GD} 
	
		\begin{itemize}
			\item \textbf{discesa stocastica del gradiente (SGD)}:\\
				è una versione modificata della discesa del gradiente che utilizza un \textit{apprendimento online}, ovvero un singolo esempio (una dimensione del batch di 1) per iterazione.
				Dato un numero sufficiente di iterazioni, SGD funziona ma è molto rumoroso. Il termine ``stocastico'' indica che l'unico esempio comprendente ciascun lotto è scelto randomicamente.
			\item \textbf{discesa stocastica del gradiente mini-batch (SGD mini-batch)}:\\
				è un compromesso tra iterazioni full-batch e SGD.
				Un mini-lotto è in genere compreso tra 10 e 1.000 esempi, scelti a caso.
				L'SGD mini-batch riduce la quantità di rumore rispetto all'SGD ma è comunque più efficiente del batch completo.
		\end{itemize}
	\end{block}

\end{frame}